# -*- coding: utf-8 -*-
"""karageorgiadis_w6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-G36cmW7kgQHvG9Kca4mGJof5mQsYYW6

# Libraries Loading
"""

import matplotlib.pyplot as plt
import numpy as np
import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras import layers

# Assure Reproducibility
from tensorflow import random
np.random.seed(1337)
random.set_seed(1337)

"""# Data Loading"""

# Model / data parameters
num_classes = 10
input_shape = (32, 32, 3)

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255
# Make sure images have shape (28, 28, 1)
# x_train = np.expand_dims(x_train, -1)
# x_test = np.expand_dims(x_test, -1)
print("x_train shape:", x_train.shape)
print(x_train.shape[0], "train samples")
print(x_test.shape[0], "test samples")
print('y before', y_test.shape)

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
print('y after', y_test.shape)

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        zca_epsilon=1e-06,  # epsilon for ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        # randomly shift images horizontally (fraction of total width)
        width_shift_range=0.1,
        # randomly shift images vertically (fraction of total height)
        height_shift_range=0.1,
        shear_range=0.,  # set range for random shear
        zoom_range=0.,  # set range for random zoom
        channel_shift_range=0.,  # set range for random channel shifts
        # set mode for filling points outside the input boundaries
        fill_mode='nearest',
        cval=0.,  # value used for fill_mode = "constant"
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False,  # randomly flip images
        # set rescaling factor (applied before any other transformation)
        rescale=None,
        # set function that will be applied on each input
        preprocessing_function=None,
        # image data format, either "channels_first" or "channels_last"
        data_format=None,
        # fraction of images reserved for validation (strictly between 0 and 1)
        validation_split=0.0)

datagen.fit(x_train)

"""# Model Training and Evaluation"""

epochs = 40
batch_size = 256
def train_and_evaluate(model):
  
  model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
  history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
  score = model.evaluate(x_test, y_test, verbose=0)
  return history, score

def train_and_evaluate_aug(model):
  
  model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
  history = model.fit_generator(datagen.flow(x_train, y_train,
                                    batch_size=batch_size),
                                    epochs=epochs,
                                    validation_data=(x_test, y_test),
                                    workers=4)
  score = model.evaluate(x_test, y_test, verbose=0)
  return history, score

def print_and_plot(history, score, name):
  print(name+" Test loss:", score[0])
  print(name+" Test accuracy:", score[1])
  plt.plot(history.history['val_accuracy'], label='validation')
  plt.plot(history.history['accuracy'], label='train')
  plt.title(name+' Accuracy')
  plt.legend()
  plt.show()
  plt.figure()
  plt.plot(history.history['val_loss'], label='validation')
  plt.plot(history.history['loss'], label='train')
  plt.title(name+' Loss')
  plt.legend()

"""# Model Building"""

# Original
model = keras.Sequential(
    [
        keras.Input(shape=input_shape),        
        layers.Conv2D(filters=32, kernel_size=3, activation="relu"), # Learn 2D Representations
        layers.Conv2D(filters=32, kernel_size=3, activation="relu"), # Learn 2D Representations
            
        layers.MaxPool2D(pool_size=2, strides=2), # Still learning 2D Representations
     
        #layers.BatchNormalization(),
        layers.Conv2D(filters=64, kernel_size=3, activation="relu"),
        layers.Conv2D(filters=64, kernel_size=3, activation="relu"),
     
        layers.MaxPool2D(pool_size=2, strides=2), # Still learning 2D Representations
     
        layers.Flatten(), # Flatten now!
        layers.Dense(512, activation="relu"),
       # layers.Dropout(0.2),

        layers.Dense(num_classes, activation="softmax"),
    ]
)

model.summary()
keras.utils.plot_model(model)

history, score = train_and_evaluate(model)

print_and_plot(history, score, "vanilla")

"""# Data Augmentation Model 1"""

history12, score12 = train_and_evaluate_aug(model)

print_and_plot(history12, score12, "vanilla_aug")

"""# Model 2 with BN, Dropout and Regularizer"""

# Original v2
model2 = keras.Sequential(
    [
        keras.Input(shape=input_shape),        
        layers.Conv2D(filters=32, kernel_size=3, activation="relu"), # Learn 2D Representations
        layers.Conv2D(filters=32, kernel_size=3, activation="relu"), # Learn 2D Representations
            
        layers.MaxPool2D(pool_size=2, strides=2), # Still learning 2D Representations
     
        layers.BatchNormalization(),
        layers.Conv2D(filters=64, kernel_size=3, activation="relu"),
        layers.Conv2D(filters=64, kernel_size=3, activation="relu"),
     
        layers.MaxPool2D(pool_size=2, strides=2), # Still learning 2D Representations
     
        layers.Flatten(), # Flatten now!
        layers.BatchNormalization(),
        layers.Dense(512, activation="relu",kernel_regularizer='l2'),
        layers.Dropout(0.4),

        layers.Dense(num_classes, activation="softmax"),
    ]
)

model.summary()
keras.utils.plot_model(model2)

history21, score21 = train_and_evaluate(model2)

print_and_plot(history21, score21, "bn_drop_vanilla")

"""# AugMentation Data on model 2 with BN(),Dropout() """

history22, score22 = train_and_evaluate_aug(model2)

print_and_plot(history22, score22, "BN_DROP_vanilla_aug")

"""# Comments
Αρχικά έφτιαξα ένα μοντελό χωρίς χρήση κάποιου batchNormalization ή Dropout layer και χωρίς χρήση regularization kernel σε Dense layer, και συγκρίνα τα αποτελέσματα με και χωρίς DataAugmentation. Παρατηρείται ότι υπάρχει βελτίωση τόσο ως προς το test accuracy όσο και προς το train καθως επίσης και στο loss όπως φαίνεται και από τα γράφηματα. Συγκερκιμένα το **valid loss** απο **2.4** πέφτει στο **0.62** ενω το accuracy από **0.71** πηγαίνει στο **0.81**. Παρόλαυτα παρατηρείται το φαινόμενο του overfitting και στις δυο περιπτώσεις για έναν αριθμό εποχών μεγαλυτερό του 6 περιπου, κρατώντας το batch_size στο *256*.

Επείτα τροποποιήσα το αρχικό μοντέλο (*Model 2*) χρησιμοποιώντας ΒΝ ,Dropout και regularization kernel στα Dense layer τα αποτελέσματα είναι ελαφρώς βελτιωμένα σε σχέση με την του αρχικού μοντέλου πάνω στα αρχικά δεδομένα , αλλά και οπως διαπιστώνουμε το gap μεταξυ του train και test (valid), από το γράφημα, είναι μικρότερο και οι καμπύλες έχουν γίνει πιο ομαλές, ενώ παρατηρείται ότι συγκλίνουν πιο γρήγορα (ειδίκα το train set). 


Συνοψήζοντας,  χρησιμοποιώντας την μέθοδο του DataAugmentation βλέπουμε πώς υπάρχει  βελτίωση στο accuracy & το loss του test set (valid ) και για τα δυο μοντέλα ενώ η χρήση regularization dropout ή batch normalization βοηθάει στην πιο εύκολη και γρήγορη εκπαίδευση ενός μοντέλου ειδικά όταν είναι αρκετά βαθύ ή πλατύ.
"""