# -*- coding: utf-8 -*-
"""Karageorgiadis_ML_w8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kz8JX6uCtTpHfqGfWAImYosiSet9znZa

Autoencoders Convolutional
"""

from tensorflow import keras
from keras import layers
from keras.models import Sequential
from keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt

# Assure Reproducibility
from tensorflow import random
np.random.seed(1337)
random.set_seed(1337)

"""Load data Normalize them"""

num_class = 10
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

_min = np.min(x_train)
_max = np.max(x_train)


#Before normalization
print(_min,_max)

# Min-Max normalization normal
x_train = (x_train-_min)/(_max -_min)
x_test = (x_test - _min) / (_max - _min)

# To categorical
y_train = keras.utils.to_categorical(y_train, num_class)
y_test = keras.utils.to_categorical(y_test, num_class)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)

# After Min-Max normalization
print(np.min(x_train),np.max(x_train))

"""#**Autoencoder** section

Encoder
"""

model_encoder = keras.models.Sequential(
    [
        keras.Input(shape=[28,28,1]),     
        
        layers.Conv2D(filters=16, kernel_size=3,  activation="relu",padding='same'), # Learn 2D Representations
        layers.MaxPooling2D(pool_size=2,strides=2),
        
        layers.Conv2D(filters=8, kernel_size=3,  activation="relu",padding='same'), # Learn 2D Representations
        layers.MaxPooling2D(pool_size=2,strides=2),

        layers.Conv2D(filters=8, kernel_size=3,  activation="relu",padding='same'), # Learn 2D Representations
        layers.MaxPooling2D(pool_size=1,strides=1),
 
    ]
)
model_encoder.summary()
keras.utils.plot_model(model_encoder, show_shapes=True)

"""Decoder"""

model_decoder = keras.models.Sequential(
    [
            
        
        layers.Conv2D(filters=8, kernel_size=3,  activation="relu",padding='same', input_shape=[7,7,8]), # Learn 2D Representations
        layers.UpSampling2D((2,2)),
        
        layers.Conv2D(filters=8, kernel_size=3,  activation="relu",padding='same'), # Learn 2D Representations
        layers.UpSampling2D((2,2)),

        layers.Conv2D(filters=16, kernel_size=3,  activation="relu",padding='same'), # Learn 2D Representations
        layers.UpSampling2D((1,1)),
     
        layers.Conv2D(filters=1, kernel_size=3,  activation="sigmoid",padding='same'),
        
        
 
    ]
)
model_decoder.summary()
keras.utils.plot_model(model_decoder, show_shapes=True)

"""Synthesize Encode Decode & Train"""

# Feed the encoder's output to the decoder, save it as a model in stacked_autoencoder
stacked_autoencoder = keras.models.Sequential([model_encoder, model_decoder])

# Compile the stacked model and train with adam
stacked_autoencoder.compile(loss="binary_crossentropy",
                   optimizer='adam')

# In autencoders the targets equal the inputs
history = stacked_autoencoder.fit(x_train, x_train,
                epochs=20,
                batch_size=256,
                validation_data=(x_test, x_test)
               )

"""Plot Loss Graph of Auto-encoder"""

plt.plot(history.history['val_loss'], label='validation')
plt.plot(history.history['loss'], label='train')
plt.title('Loss')
plt.legend()

"""#**Fully Connected Block** {Classifier}"""

input_shape = [7,7,8]

fc_block = keras.models.Sequential([
                                    keras.Input(shape=input_shape),
                                    keras.layers.Flatten(),
                                    keras.layers.Dense(128,activation='relu'),
                                    keras.layers.Dense(64,activation='relu'),
                                    # keras.layers.Dense(64,activation='relu'),
                                    keras.layers.Dense(num_class,activation="softmax"),
])

fc_block.summary()
keras.utils.plot_model(fc_block, show_shapes=True)

model_encoder.trainable= False
classifier = keras.models.Sequential([model_encoder,fc_block])

classifier.compile(loss="categorical_crossentropy",
                   optimizer='adam', metrics=["accuracy"]
                   )
history_cla = classifier.fit(x_train, y_train,
                epochs=20,
                batch_size=256,
                validation_data=(x_test, y_test)
               )

classifier.summary()

"""#**Evaluate** **Classifier** {Encoder + Classifier}"""

score = classifier.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

"""Plot Results"""

plt.plot(history_cla.history['val_accuracy'], label='validation')
plt.plot(history_cla.history['accuracy'], label='train')
plt.title('Accuracy')
plt.legend()

plt.plot(history_cla.history['val_loss'], label='validation')
plt.plot(history_cla.history['loss'], label='train')
plt.title('Loss')
plt.legend()

"""# ***Train AutoEncoder and Classifier as One*** {New Model}"""

model2 =  keras.models.Sequential(
    [
        keras.Input(shape=[28,28,1]),     
        
        layers.Conv2D(filters=16, kernel_size=3,  activation="relu",padding='same'), # Learn 2D Representations
        layers.MaxPooling2D(pool_size=2,strides=2),
        
        layers.Conv2D(filters=8, kernel_size=3,  activation="relu",padding='same'), # Learn 2D Representations
        layers.MaxPooling2D(pool_size=2,strides=2),

        layers.Conv2D(filters=8, kernel_size=3,  activation="relu",padding='same'), # Learn 2D Representations
        layers.MaxPooling2D(pool_size=1,strides=1),
     
        keras.layers.Flatten(),
        keras.layers.Dense(128,activation='relu'),
        keras.layers.Dense(64,activation='relu'),
        # keras.layers.Dense(64,activation='relu'),
        keras.layers.Dense(num_class,activation="softmax"),
 
    ]
)

model2.summary()
keras.utils.plot_model(fc_block, show_shapes=True)

model2.compile(loss="categorical_crossentropy",
                   optimizer='adam',metrics=["accuracy"])

history_tl_off = model2.fit(x_train, y_train,
                epochs=40,
                batch_size=256,
                validation_split= 0.1
               )

score = model2.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

"""Plot Accuracy"""

plt.plot(history_tl_off.history['val_accuracy'], label='validation')
plt.plot(history_tl_off.history['accuracy'], label='train')
plt.title('Accuracy')
plt.legend()

"""Plot Loss"""

plt.plot(history_tl_off.history['val_loss'], label='validation')
plt.plot(history_tl_off.history['loss'], label='train')
plt.title('Loss')
plt.legend()

"""#**Comments**

Fist I trained a Convolutional Autoencoder model using Mnist dataset for **20** *epochs* the model accuracy and loss graphs were smoother and validation with training were very close. Then I used the half-autoencoder model, by taking only the encoder part of the model and stacked it together with a fully connected block (Dense layer) in order to create a classifier. I set *encoder trainable parameters* to *'False'*, and actualy trained only the dense layers of the classifier, for another 20 epochs. The result was an accuracy of *96.6*% based on test set while the loss was *0.096*.
  
   Notice, that through Autoencoder training I used test set as validation, while through classifier trianing I used 0.1 of train set as validation in order to keep the test data for the final evaluation. Also for the training process I used *'binary_crossentropy'* for the loss function, while for the classifier I used *'categorical_crossentropy'*. 
   
   For the normalization of input data I used the min-max normalization instead of simple divide by 255 or Z-score.


# New Model

  Then I took the encoder architecture combined with the fully connected block for classification and trained the new model from start for 40 epochs. The new model converged faster than the previous (5 epochs was enough). As a result I get a higer accuracy of *98.7%* and a loss of *0.058* (0.04) based on test data during evaluation. The model took a little more time to be trained per epoch and after 5 epochs started to overfit. 

 The advatage of using the approach of encoder and classifier is that we can have a more generalized model, which by simple changing only the last few layers(fully_connected block) can be used to classify labels in a subset of the original with less effort for training the new classifier.
"""